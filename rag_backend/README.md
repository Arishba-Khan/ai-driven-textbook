---
title: Rag Chatbot
emoji: ðŸ¤–
colorFrom: blue
colorTo: indigo
sdk: docker
sdk_version: "1.0.0"
app_file: app/main.py
pinned: false
---

# RAG Chatbot Backend Service

RAG Chatbot Backend Service for Physical AI & Humanoid Robotics Textbook

## Overview

This service provides an AI-powered chatbot that can answer questions about the Physical AI & Humanoid Robotics textbook using Retrieval-Augmented Generation (RAG). The system retrieves relevant content from the textbook and generates contextual responses using a language model.

## Features

- **RAG-based Q&A**: Answers questions based on textbook content
- **Session Management**: Maintains conversation history across multiple interactions
- **Streaming Responses**: Real-time response streaming using Server-Sent Events
- **Error Handling**: Graceful degradation when services are unavailable
- **Secure Configuration**: Environment variable management for API keys

## Architecture

- **Framework**: FastAPI
- **Language Model**: Groq API with llama3 model
- **Vector Database**: Qdrant for textbook content storage and retrieval
- **Session Storage**: Neon Serverless PostgreSQL
- **Embeddings**: Cohere for generating text embeddings

## Endpoints

- `GET /health`: Health check endpoint
- `POST /chat/`: Standard chat endpoint
- `POST /chat/stream`: Streaming chat endpoint with Server-Sent Events

## Setup and Installation

1. Clone the repository
2. Create a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```
3. Install dependencies:
   ```bash
   pip install -e .
   ```
4. Create a `.env` file with your API keys based on `.env.example`:
   ```bash
   cp .env.example .env
   # Edit .env with your actual API keys
   ```
5. Start the development server:
   ```bash
   uvicorn app.main:app --reload --port 8000
   ```

## Environment Variables

- `GROQ_API_KEY`: API key for Groq LLM service
- `COHERE_API_KEY`: API key for Cohere embeddings service
- `QDRANT_API_KEY`: API key for Qdrant vector database
- `QDRANT_URL`: URL for Qdrant instance
- `QDRANT_COLLECTION_NAME`: Name of the collection storing textbook content (default: "humanoid_ai_book")
- `NEON_DB_URL`: Connection string for Neon PostgreSQL database
- `DEBUG_MODE`: Enable debug mode (default: false)
- `LOG_LEVEL`: Logging level (default: "INFO")

## API Usage

### Standard Chat

```bash
curl -X POST http://localhost:8000/chat/ \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Can you explain URDF in simple terms?",
    "session_id": "session-abc123xyz"
  }'
```

### Streaming Chat

```bash
curl -X POST http://localhost:8000/chat/stream \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Can you explain URDF in simple terms?",
    "session_id": "session-abc123xyz"
  }'
```

## Project Structure

```
rag_backend/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ pyproject.toml          # Project dependencies and configuration
â”œâ”€â”€ .env.example           # Example environment variables file
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py            # FastAPI application and server setup
â”‚   â”œâ”€â”€ core/              # Core business logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py      # Configuration and settings management
â”‚   â”‚   â”œâ”€â”€ agent_builder.py # AI agent implementation
â”‚   â”‚   â”œâ”€â”€ retriever.py   # Qdrant integration for RAG
â”‚   â”‚   â””â”€â”€ memory.py      # Session and conversation memory
â”‚   â”œâ”€â”€ api/               # API endpoints
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ chat.py        # Chat API router and endpoints
â”‚   â””â”€â”€ models/            # Pydantic data models
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ message.py     # Message data model
â”‚       â”œâ”€â”€ session.py     # Session data model
â”‚       â”œâ”€â”€ retrieval.py   # Retrieval data model
â”‚       â””â”€â”€ error.py       # Error response models
â”œâ”€â”€ api/                   # Vercel deployment entry point
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ index.py           # Mangum adapter for serverless deployment
â”œâ”€â”€ tests/                 # Test directory (structure)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ unit/
â”‚   â””â”€â”€ integration/
â””â”€â”€ vercel.json            # Vercel deployment configuration
```

## Deployment

The service is configured for deployment on Vercel. The configuration includes:

- Python 3.11 runtime
- API route handling via Mangum adapter
- Environment variable configuration for secrets

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## License

MIT License